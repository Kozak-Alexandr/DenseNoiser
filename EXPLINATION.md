# обзор уже существующих методов
Я буду рассматривать только *realtime* модели для подавления шума


## Собственно, baseline [RNNoise](https://arxiv.org/pdf/1709.08243.pdf)
То, что его выбрали в качестве *baseline* говорит уже само за себя. 
- Низкая сложность
- Реал-тайм
- Работает на CPU
- Может принимать VAD как вход для большей точности
- Барк-спектр 
- Питч-фильтр, чтобы чистить между гармониками
Классика.
![Alt-описание изображения](/путь/к/изображению)
![Alt-описание изображения](/путь/к/изображению)

## [DeepFilterNet 2](https://arxiv.org/pdf/2205.05474.pdf)
- Реалтайм на CPU (даже на Raspberry Pi)
- Верхнюю часть спектра (выше 5 кГц) обрабатывает сетка попроще, нижнюю часть – посложнее
- Верхняя часть – ERB фичи, нижняя – STFT 
- Одномерные свертки по частотам (есть двумерная свертка в энкодере), GRU – по времени
Двухэтапный процесс шумоподавления с использованием глубокой фильтрующей сетки: первый этап работает в сжатой области ERB, которая служит цели снижения вычислительной сложности при моделировании слухового восприятия человеческого уха. Весь первый этап работает в сжатой области ERB, которая служит цели снижения вычислительной сложности при моделировании слухового восприятия человеческого уха. Таким образом, целью первого этапа является улучшение огибающей речи с учетом ее грубого частотного разрешения. Второй этап работает в сложной области, используя глубокую фильтрацию [7, 6], и пытается восстановить периодичность речи.
![Alt-описание изображения](/путь/к/изображению)
![Alt-описание изображения](/путь/к/изображению)

### И тут мы начинаем понимать, что свёртки(а еще GRU) это круто

## [Conv-TasNet](https://arxiv.org/pdf/1809.07454.pdf)
Данная технология базируется на методе масок, его основа - сверточные нейронные сети Conv-TasNet
Предшественник этой архитектуры – TasNet. Архитектура TasNet [6] состоит из сверточных энкодера и декодера с некоторыми особенностями: 
выход энкодера ограничен значениями от нуля до бесконечности [0, ∞);
линейный декодер конвертирует выход энкодера в акустическую волну;
подобно многим методам-предшественникам на основе спектрограмм, на последнем этапе система аппроксимирует взвешивающую функцию (в данном случае LSTM) для каждого момента времени.
Conv-TasNet – модификация алгоритма TasNet, которая использует в качестве взвешивающей функции сверточные слои с расширением (dilation). Это модификация была сделана после того, как свертки с расширением показали себя эффективным алгоритмом при одновременном анализе и генерации данных переменной длины, в частности, для синтеза в таких решениях, как WaveNet.
![Alt-описание изображения](/путь/к/изображению)
![Alt-описание изображения](/путь/к/изображению)

# Мой любимый [DenseNet](https://arxiv.org/abs/1404.1869)
Архитектура DenseNet, или Densely Connected Convolutional Networks, представляет собой глубокую сверточную нейронную сеть, в которой каждый слой соединен с каждым предыдущим слоем внутри блока. Основная идея DenseNet заключается в том, чтобы сделать связи между слоями еще более плотными по сравнению с другими архитектурами, такими как ResNet, что позволяет эффективно передавать информацию через все уровни сети.
## Вот eё ключевые особенности:

### Блоки плотной связности (Dense Blocks): 
Основной строительный блок архитектуры DenseNet. Внутри каждого блока каждый слой принимает на вход выходы всех предыдущих слоев и передает свой выход следующему слою. Это создает крайне плотные связи между слоями.
### Сжимающие блоки (Transition Blocks): 
Используются для уменьшения размера карт признаков между блоками плотной связности. Они содержат сверточные слои и слой пулинга, что помогает уменьшить количество параметров и вычислений в сети.
### Глобальный пулинг (Global Pooling): 
В конце архитектуры обычно следует глобальный пулинг, который усредняет признаки по всему пространству, чтобы получить окончательный вектор признаков для классификации.

## Преимущества архитектуры DenseNet:

### Сокращение проблемы затухания градиента:
Благодаря коротким путям обратного распространения ошибки, обеспечиваемым плотными связями, DenseNet помогает справиться с проблемой затухания градиента в глубоких нейронных сетях.
### Эффективное использование параметров: 
Поскольку каждый слой получает входные данные от всех предыдущих слоев, нет необходимости в большом количестве параметров для передачи информации по слоям.
### Высокая точность: 
DenseNet демонстрирует хорошие результаты на различных наборах данных для классификации изображений и сегментации.

### However...Недостатки архитектуры DenseNet:

### Вычислительная сложность: 
Плотные связи между слоями могут привести к увеличению вычислительной сложности и требованиям к памяти при обучении модели(Об этом позже).
### Чувствительность к гиперпараметрам: 
Подбор гиперпараметров, таких как количество блоков и количество фильтров в блоке, может потребовать больших вычислительных ресурсов.

